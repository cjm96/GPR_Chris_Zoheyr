{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gaussian Process Regression\n",
    "\n",
    "By Christopher J. Moore & Zoheyr Doctor\n",
    "\n",
    "Prepared for two 1.5 hour practical sessions held at the \"Workshop on Reduced Order Gravitational-Wave Modeling\" at the Max Planck Institute for Gravitational Physics, Potsdam from $18^{\\textrm{th}}-22^{\\textrm{nd}}$ of June, $2018$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Useful References\n",
    "\n",
    "[RW] - The textbook \"Gaussian Processes for Machine Learning\", by Carl Edward Rasmussen and Christopher K. I. Williams, MIT Press (2006).\n",
    "\n",
    "[ADLER] - The textbook \"The Geometry of Random Fields\", by R. J. Adler, Wiley Series in Probability and Mathematical Statistics (1981).\n",
    "\n",
    "[MACKAY] - The textbook \"Information Theory, Inference, and Learning Algorithms\", by David J.C. MacKay, Cambridge University Press (2003).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1 Introduction\n",
    "\n",
    "2 What is a Gaussian Processes?\n",
    " - 2.1 Gaussian, or \"Normal\", Random Variables\n",
    " - 2.2 Gaussian Processes\n",
    " - 2.3 Some Examples of Gaussian Processes\n",
    " - - 2.3.1 Straight Line\n",
    " - - 2.3.2 Brownian Motion\n",
    " - - 2.3.2 Squared Exponential\n",
    "\n",
    "3 The Covariance Function\n",
    " - 3.1 Some General Terminology\n",
    " - 3.2 Bochner's Theorem\n",
    " - 3.3 Some Commonly Used Covariance Functions\n",
    " - - 3.3.1 SE\n",
    " - - 3.3.2 Matern\n",
    " - - 3.3.3 Wendland Polynomials\n",
    " - 3.4 Building New Covariance Functions From Old Ones\n",
    " - 3.5 Mean Square Continuity and Differentiability\n",
    "\n",
    "4 Using Gaussian Processes for Regression\n",
    " - 4.1 Regression with Gaussian Processes\n",
    " \n",
    "5 Gaussian Process Regression: A Gravitational Wave Example\n",
    "\n",
    "6 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "Regression is a general term for predicting the values of some continuous quantities.\n",
    "A typically regression problem would involve predicting the value of some smooth function at a particular point given a set of (possibly noisy) observations at some other points. \n",
    "\n",
    "As we will see in this notebook, Gaussian processes can be used in a very elegant way to tackle the regression problems. \n",
    "There are several reasons why one might want to use Gaussian process regression (GPR) over some other techniques.\n",
    "GPR is very easy to implement and makes very few hidden assumptions about the nature of the underlying function. Furthermore, the method involves the specification of relatively few arbitrary parameters (or parameterisations), and for those that are required there is very natural Bayesian framework for selecting the best parameters. The technique can be easily applied to irregularly sampled data in any number of dimensions, and is flexible enough to allow for additional data to be added later, as it becomes avaiable.\n",
    "The main disadvantage of GPR is the difficulty encoutered when trying to apply the methods to large data sets; it is typically difficult to apply the techniques that will be discussed in these notes to data sets containing more than around $10^{4}$ points. (Although generalisations of these techniques designed for large data sets do exist.)\n",
    "\n",
    "In section 2 of these notes the concept of a Gaussian process is defined. The properties of a Gaussian process are governed principally by it's covariance function and the properties of the covariance function are the topic of section 3. The remainder of the notes are concerned with how to use Gaussian processes for regression.\n",
    "\n",
    "This Jupyter notebook has been prepared to run with python 3, and we have attempted to ensure that it requires only standard libraries to run: numpy (tested with version 1.14.2), scipy (tested with version 0.19.1) and matplotlib (tested with version 2.1.0). In a small number of places the MCMC library $\\textbf{emcee}$ (tested with version 2.2.1) and the plotting library $\\textbf{corner}$ (tested with version 2.0.1) are also used. These can be installed from http://dfm.io/emcee/current/ and http://corner.readthedocs.io/en/latest/install.html respectively.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print ( \"numpy version\" , np.__version__ )\n",
    "\n",
    "import scipy\n",
    "from scipy import linalg as LA\n",
    "import scipy.optimize as spo\n",
    "print ( \"scipy version\" , scipy.__version__ )\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=True)\n",
    "print ( \"matplotlib version\" , matplotlib.__version__ )\n",
    "\n",
    "# This is only used in a small number of places, don't worry if you don't have it.\n",
    "import emcee as mc\n",
    "print ( \"emcee version\" , mc.__version__ )\n",
    "\n",
    "# This is only used in a small number of places, don't worry if you don't have it.\n",
    "import corner\n",
    "print ( \"corner version\" , corner.__version__ )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 What is a Gaussian Process?\n",
    "\n",
    "A Gaussian Process may be regarded as an infinite dimensional generalisation of the more familiar finite dimensional Gaussian random variable. Therefore we begin by recapping finite dimensional Gaussian (or \"normal\") random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Gaussian, or \"Normal\", Random Variables\n",
    "\n",
    "A continuous random variable $z\\in\\mathbb{R}$ is said to be Gaussian distributed if its probability density function (PDF) is given by\n",
    "\n",
    "$$ P(z)\\,\\textrm{d}z = \\frac{\\exp\\left(\\frac{-1}{2}\\left(\\frac{z-\\mu}{\\sigma}\\right)^{2}\\right)}{\\sqrt{2\\pi\\sigma^{2}}} \\,\\textrm{d}z \\,, \\hspace{1cm}\\textrm{(2.1)} $$\n",
    "\n",
    "where $\\mu$ is the \"mean\" and $\\sigma^{2}$ is the \"variance\";\n",
    "\n",
    "$$\\textrm{E}\\left[ z \\right] \\equiv \\int\\textrm{d}z\\,P(z)\\,z = \\mu \\,, $$\n",
    "$$\\textrm{E}\\left[ \\left(z-\\mu\\right)^{2} \\right] \\equiv \\int\\textrm{d}z\\,P(z)\\left(z-\\mu\\right)^{2} = \\sigma^{2} \\,.$$\n",
    "\n",
    "The variance must be non-negative; i.e. $\\sigma^{2}\\geq 0$.\n",
    "Eq.(2.1) is clumbersome to write, so the following shorthand is usually adopted,\n",
    "\n",
    "$$ z \\sim \\mathcal{N}(\\mu,\\sigma)\\,. $$\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "A continuous random $N$-vector $\\mathbf{z}^{\\textrm{T}}=(z_{1},z_{2},\\ldots,z_{N})\\in\\mathbb{R}^{N}$ is said to be distributed as a multivariate Gaussian if its probability density function (PDF) is given by\n",
    "\n",
    "$$ P(\\mathbf{z})\\,\\textrm{d}\\mathbf{z} = \\frac{\\exp\\left(\\frac{-1}{2}(\\mathbf{z}-\\mathbf{\\mu})^{\\textrm{T}}\\cdot\\mathbf{K}^{-1}\\cdot (\\mathbf{z}-\\mathbf{\\mu})\\right)}{\\sqrt{(2\\pi)^{N}\\left|\\mathbf{K}\\right|}} \\,\\textrm{d}\\mathbf{z} \\,, \\hspace{1cm}\\textrm{(2.2)} $$\n",
    "\n",
    "where $\\mathbf{\\mu}\\in\\mathbb{R}^{N}$ is the \"mean\" $N$-vector and $\\mathbf{K}$ is the $N\\times N$ \"covariance\" matrix;\n",
    "\n",
    "$$\\textrm{E}\\left[ \\mathbf{z} \\right] \\equiv \\int\\textrm{d}\\mathbf{z}\\,P(\\mathbf{z})\\mathbf{z} = \\mathbf{\\mu} \\,, $$\n",
    "$$\\textrm{E}\\left[ \\left(\\mathbf{z}-\\mathbf{\\mu}\\right)\\left(\\mathbf{z}-\\mathbf{\\mu}\\right)^{\\textrm{T}} \\right] \\equiv \\int\\textrm{d}\\mathbf{z}\\,P(\\mathbf{z})\\left(\\mathbf{z}-\\mathbf{\\mu}\\right)\\otimes\\left(\\mathbf{z}-\\mathbf{\\mu}\\right)^{\\textrm{T}} = \\mathbf{K} \\,. $$\n",
    "\n",
    "Just as we had $\\sigma^{2}\\geq 0$ previously, here the covariance matrix must always be symmetric (i.e. $\\mathbf{K}=\\mathbf{K}^{\\textrm{T}}$) and $\\textbf{positive semi-definite}$. Eq.(2.2) is clumbersome to write, so the following shorthand is usually adopted,\n",
    "\n",
    "$$ \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu},\\mathbf{K})\\,. $$\n",
    "\n",
    "$\\textbf{Definition: }$ A real $N\\times N$ matrix $\\mathbf{K}$ is $\\textbf{positive semi-definite}$ if $\\mathbf{u}^{\\textrm{T}}\\cdot\\mathbf{K}\\cdot\\mathbf{u}\\geq 0$ for any $N$-vector $\\mathbf{u}\\in\\mathbb{R}^{N}$.\n",
    "\n",
    "Furthermore, if $\\mathbf{u}^{\\textrm{T}}\\cdot\\mathbf{K}\\cdot\\mathbf{u} = 0 \\Rightarrow \\mathbf{u}=\\mathbf{0}$ then $\\mathbf{K}$ is said to be strictly positive definite.\n",
    "\n",
    "$\\textbf{Exercise: }$ Show that a symmetric matrix is positive semi-definite if and only if all of its eigenvalues are non-negative.\n",
    "\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "We will assume the reader is familiar with most of the properties of multivariate Gaussian distributions. However, we will point out that a Gaussian distribution is fully specified by just the mean $\\mathbf{\\mu}$ and the pairwise covariances $\\mathbf{K}$ (all the higher order moments, e.g. $\\textrm{E}\\big[z_{1}\\times z_{2} \\times z_{3}\\big]$, may be expressed in terms of just $\\mathbf{\\mu}$ and $\\mathbf{K}$). We also poinrt our the \"affine\" property of Gaussian random variables which will be relevant in what follows; if $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu},\\mathbf{K})$ then for any $M$-vector $\\mathbf{a}$ and $M\\times N$ matrix $b$, the vector $\\mathbf{y}=\\mathbf{a}+\\mathbf{b}\\cdot\\mathbf{z}$ is also a Gaussian random variable, $\\mathbf{y}\\sim\\mathcal{N}(\\mathbf{a}+\\mathbf{b}\\cdot\\mathbf{\\mu},\\mathbf{b}\\cdot\\mathbf{K}\\cdot\\mathbf{b}^{\\textrm{T}})$. In particular, this implies that any subset of the components of $\\mathbf{z}$ is also a Gaussian random variable and the covariance matrix is given by the relevant subset of the components of $\\mathbf{K}$.\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "It is easy to verify that the random $N$-vector $\\mathbf{z}'=\\mathbf{z}-\\mathbf{\\mu}$ is distributed as $\\mathbf{z}' \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{\\Sigma})$. For this reason, it is usually possible to (without loss of generality) choose to work with zero-mean Gaussian random variables.\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "Gaussian random variables are well suppported in any sensible programming language; e.g. python$\\ldots$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD7CAYAAABpJS8eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTBD4AJAJAh8AMkHgA0AmCHwAyMT/AawUEJuVC7TyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fe859b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finite dimensional Gaussian random variables in python\n",
    "\n",
    "# A mean 2-vector and a 2 by 2 covariance matrix\n",
    "mu = np.array ( [ 0 , 0 ] )\n",
    "K = np.array ( [ [ 1 , 3 ] , [ 3 , 10 ] ] )\n",
    "\n",
    "# Draw 1000 realisations of the Gaussian random variable z~N(mu,K)\n",
    "dist = np.array ( [ np.random.multivariate_normal ( mu , K ) for i in range ( 1000 ) ] )\n",
    "\n",
    "# Plot the distribution of z\n",
    "plt.hist2d ( dist[:,0] , dist[:,1] , bins=40 )\n",
    "plt.show ( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gaussian Processes\n",
    "\n",
    "Instead of distributions over a finite number of random variables (i.e. random vectors) we now wish to consider distributions over smooth random functions. For some set $\\mathcal{S}$ we will consider real valued functions $\\,f:\\mathcal{S}\\rightarrow\\mathbb{R}$. The random variables will now be the values of the function evaluated at some particular input; i.e. $f(x)$ for $x\\in\\mathcal{S}$.\n",
    "\n",
    "We will use the following definition of a Gaussian process (GP).\n",
    "\n",
    "$\\textbf{Definition }$ For any set $\\mathcal{S}$, a $\\textbf{Gaussian Process}$ on $\\mathcal{S}$ is a collection of random variables $\\{{f(x):x\\in \\mathcal{S}\\}}$ such that for any finite ${n\\in\\mathbb{N}}$, and any ${x_{1},x_{2},\\ldots,x_{n}\\in\\mathcal{S}}$, the vector ${\\left(f(x_{1}),f(x_{2}),\\ldots,f(x_{n})\\right)^{\\mathrm{T}}\\in\\mathbb{R}^{n}}$ is distributed as a multivariate Gaussian.\n",
    "\n",
    "Notice that in this definition the set $\\mathcal{S}$ can be either finite and infinite dimensional. However, the finite case is of little interest as this definition simply reduces to the familiar multivariate Gaussian discussed above.\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "$\\textbf{An Example of a GP: }$ For this audience perhaps the most familiar example of a Gaussian process is the noise, $n(t)$, in a gravitational wave interferometer. \n",
    "The noise is a real valued function of time. \n",
    "The noise is usually assumed to be stationary and Gaussian, meaning that in the frequency domain $\\textrm{E}\\left[ {\\widetilde{n}} (f) {\\widetilde{n}} (f')^{*}\\right]=\\frac{1}{2}S_{n}(f)\\delta(f-f')$. \n",
    "From the definition of the Fourier transform, it follows that in the time domain $\\textrm{E}\\left[n(t)n(t')\\right]=\\int_{0}^{\\infty}S_{n}(f)\\cos\\left(2\\pi f(t-t')\\right) \\equiv \\kappa(t-t')$, using the Fourier transform conventions of [arXiv:1408.0740]. The function $S_{n}(f)$ is called the detector's one-sided noise power spectral density (usually given in units of $\\textrm{Hz}^{-1}$).\n",
    "Comparing with our above definition we see that the noise is a GP with $\\mathcal{S}=\\mathbb{R}$, $x=t$, and $z(x)=n(t)$. \n",
    "The vector of noise values observed at any finite collection of time values, $\\mathbf{n}^{\\mathrm{T}}\\equiv\\left(n(t_{1}),n(t_{2}),\\ldots,n(t_{N})\\right)$, is distributed as $\\mathbf{n}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{K})$, where\n",
    "\n",
    "$$\\mathbf{K} = \\begin{pmatrix} \\kappa(t_{1}-t_{1}) & \\kappa(t_{1}-t_{2}) & \\ldots & \\kappa(t_{1}-t_{N}) \\\\ \\kappa(t_{2}-t_{1}) & \\kappa(t_{2}-t_{2}) & \\ldots & \\kappa(t_{2}-t_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\kappa(t_{N}-t_{1}) & \\kappa(t_{N}-t_{2}) & \\ldots & \\kappa(t_{N}-t_{N})  \\end{pmatrix}\\,.$$\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "Fortunately, it is very easy construct new GPs with any desired properties by simply specifying the desired mean ($\\mu(x)=\\textrm{E}\\big[f(x)\\big]$) and pairwise covariance ($k(x,x')=\\textrm{E}\\Big[\\big(f(x)-\\mu(x)\\big)\\big(f(x')-\\mu(x')\\big)\\Big]$). We are free to choose any mean function, $\\mu:\\mathcal{S}\\rightarrow\\mathbb{R}$; just as with Gaussian distributions, we will usually work with zero-mean GPs, $\\mu(x)=0$ for all $x\\in\\mathcal{S}$. We are also free to choose any \"covariance function\", $k:\\mathcal{S}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$, subject to some constraints to be discussed below. Extending the notation in equations (1.1) and (1.2), if a function $f(x)$ is drawn from a GP with mean function $\\mu(x)$ and covariance function $k(x,x')$ then we write\n",
    "\n",
    "$$f(x) \\sim \\mathcal{GP}( \\, \\mu(x) \\, , \\, k(x,x') \\, ) \\,. \\hspace{3cm} \\textrm{(2.3)}$$\n",
    "\n",
    "From the definition of a GP, for any finite set of points $\\mathbf{x}_{*}^{\\textrm{T}} = (x_{1},x_{2},\\ldots,x_{N})$ the $N$-vector $\\mathbf{f}_{*}^{\\textrm{T}}=\\left(f(x_{1}),f(x_{2}),\\ldots,f(x_{N})\\right)$ must distributed as $\\mathbf{f}_{*} \\sim \\mathcal{N}( \\mathbf{M} , \\mathbf{K} )$, where\n",
    "\n",
    "$$ \\begin{align} \n",
    "&\\mathbf{M}^{\\textrm{T}} = \\big( \\, \\mu(x_{1}) \\, , \\, \\mu(x_{2}) \\, , \\ldots , \\, \\mu(x_{N}) \\, \\big) \\,, \\hspace{2cm} &\\textrm{(2.4)} \\\\\n",
    "\\textrm{and } \\; &\\mathbf{K} =  \\begin{pmatrix} k(x_{1},x_{1}) & k(x_{1},x_{2}) & \\ldots & k(x_{1},x_{N}) \\\\ k(x_{2},x_{1}) & k(x_{2},x_{2}) & \\ldots & k(x_{2},x_{N}) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ k(x_{N},x_{1}) & k(x_{N},x_{2}) & \\ldots & k(x_{N},x_{N})  \\end{pmatrix} \\,. \\hspace{2cm} &\\textrm{(2.5)}\n",
    "\\end{align} $$\n",
    "\n",
    "If this is to make any sense, then the matrix $\\mathbf{K}$ formed in equation (2.5) had better be a valid be symmetric positive semi-definite covariance matrix. To ensure that this is always the case the covariance function $k(x,x')$ must be a $\\textbf{symmetric}$ function (i.e. $k(x,x')=k(x',x)$) and a $\\textrm{positive semi-definite}$ function.\n",
    "\n",
    "$\\textbf{Definition: }$ A covariance function $k:\\mathcal{S}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$ is $\\textbf{positive semi-definite}$ if the $N\\times N$ matrix $\\mathbf{K}_{ij}=k(x_{i},x_{j})$ is positive semi-definite for all sets of points $x_{1},x_{2},\\ldots,x_{n}\\in\\mathcal{S}$ and for all $N\\in\\mathbb{N}$.\n",
    "\n",
    "In the context of Gaussian Process Regression (GPR), covariance functions are sometimes also known as \"kernels\".\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "Note that equation (2.3) defines a Gaussian process by specifying the mean and pairwise covariance between random variables. Defining the GP in this way ensures that internal inconsistancies can never arrise among any of the random variables in the GP. For example, if our GP specifies that for a pair of points $x_{1}$ and $x_{1}$ we have $(f(x_1),f(x_2))\\sim\\mathcal{N}\\big(\\left[0,0\\right],\\left[\\left[k(x_1,x_1),k(x_1,x_2)\\right],\\left[k(x_2,x_1),k(x_2,x_2)\\right]\\right]\\big)$, then the affine property of Gaussian distributions automatically ensures that $f(x_1)\\sim\\mathcal{N}\\big(0,k(x_1,x_1)\\big)$ and $f(x_2)\\sim\\mathcal{N}\\big(0,k(x_2,x_2)\\big)$, as required by the definition in equation (2.3). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Some Examples of Gaussian Processes\n",
    "\n",
    "### 2.3.1 Straight Line\n",
    "\n",
    "Consider the Gaussian process on the real line ($\\mathcal{S}=\\mathbb{R}$) with $\\mu(x) = 0$ and $k_{\\textrm{line}}(x,x')=xx'$.\n",
    "\n",
    "To show that this is indeed a valid GP we need to show that $k_{\\textrm{line}}(x,x')$ is a symmetric positive semi-definite covariance function. It is obviously symmetric, but the positive definite property requires a proof. Consider the covariance matrix formed by acting $k_{\\textrm{line}}$ pairwise on any finite set of points $\\mathbf{x}=(x_{1},x_{2},\\ldots,x_{n})$; the covariance matrix is $\\mathbf{K}=\\mathbf{x}\\otimes\\mathbf{x}^{\\textrm{T}}$, or $\\mathbf{K}_{ij}=\\mathbf{x}_{i}\\mathbf{x}_{j}$. To show that this matrix is positive semi-definite consider the sum $s=\\mathbf{u}^{\\textrm{T}}\\cdot\\mathbf{K}\\cdot\\mathbf{u}$, for some arbitrary $n$-vector $\\mathbf{u}\\in\\mathbb{R}^{n}$. The sum may be rewritten as $s=\\left(\\mathbf{u}^{\\textrm{T}}\\cdot\\mathbf{x}\\right)^{2}$ which shows that $s\\geq0$, and hence that $\\mathbf{K}$ is a positive semi-definite matrix. Since this holds for any choice of the points $\\mathbf{x}$ it follows that $k_{\\textrm{line}}(x,x')$ is a positive semi-definite function.\n",
    "\n",
    "Alternatively, we may show that $k_{\\textrm{line}}(x,x')$ is positive semi-definite by considering the eigenvalues of the symmetric covariance matrix $\\mathbf{K}=\\mathbf{x}\\otimes\\mathbf{x}^{\\textrm{T}}$. This matrix has one non-zero eigenvalue $\\lambda=\\left|\\mathbf{x}\\right|^{2} \\geq 0$ corresponding to the eigenvector $\\mathbf{x}$ (all the other eigenvectors vanish). Because all of the eigenvalues are non-negative for any choice of points $\\mathbf{x}=(x_{1},x_{2},\\ldots,x_{n})$, and for any $n\\in\\mathbb{N}$, the covariance function $k_{\\textrm{line}}(x,x')$ is positive semi-definite.\n",
    "\n",
    "We are now in a position to draw a random realisation from this GP. The code in the following cell defines the functions $\\mu(x)$, $k_{\\textrm{line}}(x,x')$, and the vector $\\mathbf{x}_{*}$ which is chosen to be a densely sampled vector of points in the range $0\\leq x < 10$. Try running the following cell several times to get a sense of the behaviour of this GP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181d421940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Straight Line\n",
    "\n",
    "# The mean function of the GP\n",
    "def mu ( x ):\n",
    "    return 0\n",
    "\n",
    "# The covariance function of the GP\n",
    "def k_line ( x , y ):\n",
    "    return x * y\n",
    "\n",
    "# The sampling points\n",
    "x_star = np.arange ( 0 , 10 , 0.5 )\n",
    "\n",
    "# The mean N-vector - formed from the mean function, mu\n",
    "M = np.array ( [ mu(x) for x in x_star ] )\n",
    "\n",
    "# The N by N covariance matrix - formed from the covariance function, k.\n",
    "K = np.array( [ [ k_line(x,y) for x in x_star ] for y in x_star ] )\n",
    "\n",
    "# A random realisation of the function values - drawn from the multivariate Gaussian distribution ~ N(M,K)\n",
    "f_star = np.random.multivariate_normal ( M , K )\n",
    "\n",
    "# Plot a realisation of the GP\n",
    "plt.plot ( x_star , f_star , 'bo' , markersize=4 )\n",
    "plt.ylim ( -14 , 14 )\n",
    "plt.show ( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise:}$ If you were to run the above cell many times and record the gradients of lines obtained, what distribution would the gradients follow? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Brownian Motion\n",
    "\n",
    "Consider the Gaussian process on the positive real line ($\\mathcal{S}=\\mathbb{R}_{+}$) with $\\mu(x) = 0$ and $k_{\\textrm{Brownian}}(x,x')=\\textrm{min}(x,x')$.\n",
    "\n",
    "To show that this is a valid GP we need to show that $k_{\\textrm{Brownian}}(x,x')$ is a symmetric positive semi-definite covariance function. As was the case in the previous example 2.3.1 the symmetry property is obvious, but the positive semi-definite property requires proof. \n",
    "\n",
    "$\\textbf{Exercise:}$ Show that $k_{\\textrm{Brownian}}(x,x')$ is a positive semi-definite function. \n",
    "\n",
    "$\\textbf{Hint: }$ Let $H(t;x)=\\begin{cases}1\\quad\\textrm{if }t\\leq x\\\\0\\quad\\textrm{else}\\end{cases}\\,,$ and write the $\\textrm{min}$ function as $\\textrm{min}(x_{i},x_{j})=\\int_{0}^{\\infty}\\textrm{d}t\\;H(t;x_{i})H(t;x_{j})\\,.$ \n",
    "\n",
    "We are now in a position to draw a random realisation from this GP. The code in the following cell defines the functions $\\mu(x)$, $k_{\\textrm{line}}(x,x')$, and the vector $\\mathbf{x}_{*}$ which is chosen to be a densely sampled vector of points in the range $0\\leq x < 10$. Try running the following cell several times to get a sense of the behaviour of this GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD7CAYAAABpJS8eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181d4e1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Brownian Motion\n",
    "\n",
    "# The mean function of the GP\n",
    "def mu ( x ):\n",
    "    return 0\n",
    "\n",
    "# The covariance function of the GP\n",
    "def k_Brownian ( x , y ):\n",
    "    return min ( x , y )\n",
    "\n",
    "# The sampling points\n",
    "x_star = np.arange ( 0 , 10 , 0.15 )\n",
    "\n",
    "# The mean N-vector - formed from the mean function, mu\n",
    "M = np.array ( [ mu(x) for x in x_star ] )\n",
    "\n",
    "# The N by N covariance matrix - formed from the covariance function, k.\n",
    "K = np.array( [ [ k_Brownian(x,y) for x in x_star ] for y in x_star ] )\n",
    "\n",
    "# A random realisation of the function values - drawn from the multivariate Gaussian distribution ~ N(M,K)\n",
    "f_star = np.random.multivariate_normal ( M , K )\n",
    "\n",
    "# Plot a realisation of the GP\n",
    "plt.plot ( x_star , f_star , linewidth=1 , marker='o' , markersize=4 , color='b' )\n",
    "plt.show ( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Squared Exponential\n",
    "\n",
    "Consider the Gaussian process on the real line ($\\mathcal{S}=\\mathbb{R}$) with $\\mu(x) = 0$ and $k_{\\textrm{SE}}(x,x')=\\exp\\left(-\\frac{1}{2}(x-x')^{2}\\right)$. This is a special case of the widely used \"squared exponential\" covariance function (see section 3.3.1).\n",
    "\n",
    "The function $k_{\\textrm{SE}}(x,x')$ is also symmetric positive semi-definite (see section 3.2).\n",
    "\n",
    "We are now in a position to draw a random realisation from this GP. The code in the following cell defines the functions $\\mu(x)$, $k_{\\textrm{line}}(x,x')$, and the vector $\\mathbf{x}_{*}$ which is chosen to be a densely sampled vector of points in the range $0\\leq x < 10$. Try running the following cell several times to get a sense of the behaviour of this GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD7CAYAAACMlyg3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181d2f5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Squared exponential\n",
    "\n",
    "# The mean function of the GP\n",
    "def mu ( x ):\n",
    "    return 0\n",
    "\n",
    "# The covariance function of the GP - this is an example of a \"squared exponential\" covariance function, see Sec.?\n",
    "def k_SE ( x , y ):\n",
    "    return np.exp ( -0.5 * (x-y) * (x-y) )\n",
    "\n",
    "# The sampling points\n",
    "x_star = np.arange ( 0 , 10 , 0.1 )\n",
    "\n",
    "# The mean N-vector - formed from the mean function, mu\n",
    "M = np.array ( [ mu(x) for x in x_star ] )\n",
    "\n",
    "# The N by N covariance matrix - formed from the covariance function, k.\n",
    "K = np.array( [ [ k_SE(x,y) for x in x_star ] for y in x_star ] )\n",
    "\n",
    "# A random realisation of the function values - drawn from the multivariate Gaussian distribution ~ N(M,K)\n",
    "f_star = np.random.multivariate_normal ( M , K )\n",
    "\n",
    "# Plot a realisation of the GP\n",
    "plt.plot ( x_star , f_star , linewidth=1 , color='b' )\n",
    "plt.show ( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise:}$ The above cell generates random realisations of a squared exponential covariance function. Try modifying the covariance function to $k_{\\textrm{SE}}(x,x')=\\sigma_{f}^{2}\\exp\\left(\\frac{-1}{2}\\frac{(x-x')^{2}}{\\ell^{2}}\\right)$ and experiment with different choices of $\\sigma_{f}$ and $\\ell$. Explore how the these two \"hyperparameters\" (see Sec.3) control the properties of the GP. (You can also experiment with using a non-zero mean function, $\\mu(x)$.) Also, try experimenting with using $k_{\\textrm{?}}(x,x')=\\exp\\left(\\frac{-1}{2}(x-x')^{3}\\right)$; why doesn't this work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the examples in sections $2.3.1$ and $2.3.1$ had a special point at $x=0$. The squared-exponential does not have any such prefered point, it is in fact translation invariant; $k(x,x')=k(x+\\Delta,x'+\\Delta)$. A covariance function with this property is called \"stationary\" (see section 3.1).\n",
    "\n",
    "Note also that in the example in section (2.3.3) we have choosen to stop drawing the curve with points and instead use a smooth curve. Of course, any practical computation involving GPs is performed on a finite number of points, so a list plot is more honest. However, the choice of the points $\\mathbf{x}_{*}$ is arbitrary and is designed to show the smooth behaviour of the underlying GP, so perhaps the line plot is more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 The Covariance Function\n",
    "\n",
    "In section 2 it was shown how a GP can be specified by a symmetric positive definite covariance function, $k(x,x')$. We also saw through several examples that this covariance function governs the properties of the GP. In this section several covariance functions which are commonly used for regression will be introduced in a more systematic way. In addition, a more formal discussion of how the covariance function governs the \"smoothness\" of the GP will also be presented in section 3.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Some General Terminology\n",
    "\n",
    "The discussion in this section will be restricted to covariance functions of GPs on $\\mathcal{S}=\\mathbb{R}^{N}$. In what follows $x,x'\\in\\mathbb{R}^{N}$, angled brackets $\\left<x|x'\\right>$ denote the standard Euclidean inner product between the $N$-vectors $x$ and $x'$, and $|x|=\\sqrt{\\left<x|x\\right>}$ denotes the standard Euclidean norm of $x$.\n",
    "\n",
    "$\\textbf{Definition: }$ A covariance function is said to be $\\textbf{stationary}$ if it depends only on the difference of the inputs; i.e. $k(x,x')=k(\\tau)$, where $\\tau=x-x'$. A stationary covariance function is invariant under translations; i.e. $k(x,x')=k(x+\\Delta,x'+\\Delta)$.\n",
    "\n",
    "$\\textbf{Definition: }$ A stationary covariance function is said to be $\\textbf{isotropic}$ if it is only a function of the Euclidean distance between the inputs; $k(x,x')=k(r)$, where $r=\\left|x-x'\\right|$. An isotropic covariance function is invariant under rotations as well as translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Bochner's Theorem\n",
    "\n",
    "Section 2 introduced several examples of covariance functions and in each case it was necessary to prove that proposed covariance function was symmetric and positive semi-definite. The proof was different in each case, and in general there is no easy way to establish whether a particular function is positive semi-definite. (Although, it is usually easy to prove that a particular function is NOT positive semi-definite by simply evaluating the covariance matrix formed from a particular set of points and showing that this matrix has at least one negative eigenvalue.)\n",
    "\n",
    "$\\textbf{Exercise: }$ Show that the function $k(x,x')=\\begin{cases} \\exp(-(x-x')^{2}) &\\;\\textrm{if }|x-x'|<1 \\\\ 0 &\\;\\textrm{else} \\end{cases}$ is NOT positive semi-definite.\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "Fortunately there is a theorem which allows us to easy establish whether any $\\textbf{stationary}$ function, $k(\\tau)$, is positive definite.\n",
    "\n",
    "$\\textbf{Bochner's Theorem: }$ A function $k:\\mathcal{S}\\rightarrow\\mathbb{R}$ is the covariance function of a stationary mean square continuous GP on $\\mathbb{R}^{N}$ if and only if it can be written as the following integral using a positive finite function $S(x)$ on $\\mathbb{R}^{N}$;\n",
    "$$k(\\tau) = \\int_{\\mathbb{R}^{N}}\\textrm{d}x \\; S(x) \\exp\\big(2\\pi i \\left<x|\\tau\\right> \\big) \\,. $$\n",
    "\n",
    "The function $S(x)$ is simply the Fourier transform of $k(\\tau)$. \n",
    "\n",
    "In section 2 the squared exponential covariance function was introduced. Bochner's theorem may now be used to prove that this function is indeed positive definite. The Fourier transform of the squared exponential function is another squared exponential function, which is always positive. Therefore, by Bochner's theorem, the function $k_{SE}$ is positive definite. \n",
    "\n",
    "$\\textbf{Exercise: }$ Show for a second time that the function $k(x,x')=\\begin{cases} \\exp(-(x-x')^{2}) &\\;\\textrm{if }|x-x'|<1 \\\\ 0 &\\;\\textrm{else} \\end{cases}$ is NOT positive semi-definite, this time using Bochner's theorem.\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Some Commonly Used Covariance Functions\n",
    "\n",
    "In this section we will list some covariance functions for GPs on $\\mathcal{S}=\\mathbb{R}^{N}$ that have been widely used in the literature for regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Squared-Exponential Covariance\n",
    "\n",
    "Let's start with probably the most widely used kernel, the squared exponential, which has the form:\n",
    "\n",
    "$$k_{\\textrm{SE}}(x,x') = \\exp\\left(-\\frac{1}{2}\\frac{||x-x'||^2}{l^2}\\right)\\,.$$\n",
    "\n",
    "A slightly more general (and more flexible) version uses a non-trivial distance metric (Einstein summation convention is used here):\n",
    "\n",
    "$$k_{\\textrm{SE}}(x,x') = \\exp\\left(-\\frac{1}{2}g_{ab}(x_a-x'_a)(x_b-x'_b)\\right)\\,.$$\n",
    "\n",
    "The constants $l$ or the components of $g_{ab}$ are parameters to be chosen by the user. Alternatively, these parameters can be optimized for (see section X).\n",
    "\n",
    "In the code cell below a squared exponential kernel method that will compute the $K$ matrix with $k({\\bf x},{\\bf x_*})$ between points $\\texttt{X1}$ and $\\texttt{X2}$ has been defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_exponential_kernel ( X1 , X2 , scale=1.0 , metric=None ):\n",
    "    \"\"\"\n",
    "    Computes the square exponential kernel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X1: array, shape = (npoints,ndim)\n",
    "        Array of points in parameter space for which\n",
    "        to calculate the squared exponential covariance\n",
    "        with points in X2\n",
    "    X2: array, shape = (npoints,ndim)\n",
    "        Array of points in parameter space for which\n",
    "        to calculate the squared exponential covariance\n",
    "        with points in X1\n",
    "    scale: float or array with length ndim\n",
    "        Squared exponential length scale \n",
    "    metric: callable\n",
    "        A function that computes distances between\n",
    "        two points.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    K: matrix\n",
    "        The covariance matrix for points in X\n",
    "    \"\"\"\n",
    "    npoints1 = X1.shape[0]\n",
    "    npoints2 = X2.shape[0]\n",
    "    ndim = X1.shape[1]\n",
    "    if isinstance(scale,(float,int)):\n",
    "        inv_scale = (1./scale)*np.ones(ndim)\n",
    "        inv_scale = np.diag(inv_scale)\n",
    "    else:\n",
    "        inv_scale = 1./scale\n",
    "        inv_scale = np.diag(inv_scale)\n",
    "    if metric == None:\n",
    "        dist = lambda a,b: np.sqrt(np.dot(np.dot(a-b,inv_scale),a-b))\n",
    "    else:\n",
    "        dist = metric\n",
    "    K = np.empty([npoints1,npoints2])\n",
    "    for i in range(npoints1):\n",
    "        for j in range(npoints2):\n",
    "            try:\n",
    "                K[i,j] = np.exp(-0.5*(dist(X1[i,:],X2[j,:])**2))\n",
    "            except FloatingPointError:\n",
    "                K[i,j]=0.\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Matern Covariance\n",
    "\n",
    "When working with an isotropic covariance function (including the squared exponential described above in section 3.3.1) it is often convenient to define the scaled distance between two points $x,x'\\in\\mathcal{S}$;\n",
    "\n",
    "$$ r(x,x') = \\frac{|x-x'|}{l} \\,. $$\n",
    "\n",
    "Or a slightly more general version using a non-trivial distance metric;\n",
    "\n",
    "$$ r(x,x') = \\sqrt{g_{ab}(x_a-x'_a)(x_b-x'_b)} \\,. $$\n",
    "\n",
    "Following the conventions in [RW] the Matern class of kernels are defined as:\n",
    "\n",
    "$$ k_{\\textrm{Matern}}(r) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu}r\\right)^{\\nu} K_\\nu\\left(\\sqrt{2\\nu}r\\right)\\,. $$\n",
    "\n",
    "The parameters $\\nu$ and $l$ are positive numbers and $K_\\nu(x)$ denotes the modified Bessel functions of the first kind.  While any value of $\\nu$ can be chosen in principle, one often chooses $\\nu$ to be a half integer value, because the covariance can then be written in the somewhat simpler form,\n",
    "\n",
    "$$ k_{\\nu=p+1/2}(r) = \\exp\\left(\\sqrt{2\\nu}r\\right) \\frac{\\Gamma(p+1)}{\\Gamma(2p+1)} \\sum_{i=0}^{p} \\frac{(p+i)!}{i!(p-i)!}  \\left(\\sqrt{8\\nu}r\\right)^{p-i}\\,. $$\n",
    "\n",
    "[RW] claim that the $\\nu=3/2$ and $\\nu=5/2$ kernels are the most important for machine learning, and they also have the added benefit of having even simpler forms:\n",
    "\n",
    "$$ k_{\\nu = 3/2}(r) = \\left(1+\\sqrt{3}r\\right) \\exp\\left(-\\sqrt{3}r\\right) \\,, $$\n",
    "\n",
    "$$ k_{\\nu = 5/2}(r) = \\left(1+\\sqrt{5}r + \\frac{5r^2}{3} \\right) \\exp\\left(-\\sqrt{5}r\\right) \\,. $$\n",
    "\n",
    "Perhaps the most interesting aspect of the Matern kernels is that the level of smoothness of the underlying GP can be controlled via the parameter $\\nu$. In fact, the GP is $n$-times mean-square differentiable where $n<\\nu$ (see section 3.5 on mean square differentiability). As $\\nu\\rightarrow\\infty$, a GP described by the Matern kernels become infinitely mean square differentiable. In fact, as $\\nu\\rightarrow\\infty$ the Matern kernel actually recovers the squared-exponential kernel! \n",
    "\n",
    "In the code cell below the Matern 3/2 and Matern 5/2 kernels have been defined as Python methods that will compute the $K$ matrix with $k({\\bf x},{\\bf x_*})$ between points $\\texttt{X1}$ and $\\texttt{X2}$ has been defined. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Matern32 ( X1 , X2 , scale=1.0 , metric=None ):\n",
    "    \"\"\"\n",
    "    Computes Matern 3/2 kernel\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X1: array, shape = (npoints,ndim)\n",
    "        Array of points in parameter space for which\n",
    "        to calculate the squared exponential covariance\n",
    "        with points in X2\n",
    "    X2: array, shape = (npoints,ndim)\n",
    "        Array of points in parameter space for which\n",
    "        to calculate the squared exponential covariance\n",
    "        with points in X1\n",
    "    scale: float or array with length ndim\n",
    "        Squared exponential length scale \n",
    "    metric: callable\n",
    "        A function that computes distances between\n",
    "        two points.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    K: matrix\n",
    "        The covariance matrix for points in X\n",
    "    \"\"\"\n",
    "    npoints1 = X1.shape[0]\n",
    "    npoints2 = X2.shape[0]\n",
    "    ndim = X1.shape[1]\n",
    "    if isinstance(scale,(float,int)):\n",
    "        inv_scale = (1./scale)*np.ones(ndim)\n",
    "        inv_scale = np.diag(inv_scale)\n",
    "    else:\n",
    "        inv_scale = 1./scale\n",
    "        inv_scale = np.diag(inv_scale)\n",
    "    if metric == None:\n",
    "        dist = lambda a,b: np.sqrt(np.dot(np.dot(a-b,inv_scale),a-b))\n",
    "    else:\n",
    "        dist = metric\n",
    "    K = np.empty([npoints1,npoints2])\n",
    "    for i in range(npoints1):\n",
    "        for j in range(npoints2):\n",
    "            try:\n",
    "                K[i,j] = (1.+np.sqrt(3)*dist(X1[i,:],X2[j,:]))*np.exp(-np.sqrt(3)*dist(X1[i,:],X2[j,:]))\n",
    "            except FloatingPointError:\n",
    "                K[i,j]=0.\n",
    "    return K\n",
    "\n",
    "def Matern52 ( X1 , X2 , scale=1.0 , metric=None ):\n",
    "    \"\"\"\n",
    "    Computes Matern 5/2 kernel\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X1: array, shape = (npoints,ndim)\n",
    "        Array of points in parameter space for which\n",
    "        to calculate the squared exponential covariance\n",
    "        with points in X2\n",
    "    X2: array, shape = (npoints,ndim)\n",
    "        Array of points in parameter space for which\n",
    "        to calculate the squared exponential covariance\n",
    "        with points in X1\n",
    "    scale: float or array with length ndim\n",
    "        Squared exponential length scale \n",
    "    metric: callable\n",
    "        A function that computes distances between\n",
    "        two points.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    K: matrix\n",
    "        The covariance matrix for points in X\n",
    "    \"\"\"\n",
    "    npoints1 = X1.shape[0]\n",
    "    npoints2 = X2.shape[0]\n",
    "    ndim = X1.shape[1]\n",
    "    if isinstance(scale,(float,int)):\n",
    "        inv_scale = (1./scale)*np.ones(ndim)\n",
    "        inv_scale = np.diag(inv_scale)\n",
    "    else:\n",
    "        inv_scale = 1./scale\n",
    "        inv_scale = np.diag(inv_scale)\n",
    "    if metric == None:\n",
    "        dist = lambda a,b: np.sqrt(np.dot(np.dot(a-b,inv_scale),a-b))\n",
    "    else:\n",
    "        dist = metric\n",
    "    K = np.empty([npoints1,npoints2])\n",
    "    for i in range(npoints1):\n",
    "        for j in range(npoints2):\n",
    "            try:\n",
    "                K[i,j] = (1.+np.sqrt(5)*dist(X1[i,:],X2[j,:])+\n",
    "                          (5./3)*(dist(X1[i,:],X2[j,:]))**2.)*np.exp(-np.sqrt(5)*dist(X1[i,:],X2[j,:]))\n",
    "            except FloatingPointError:\n",
    "                K[i,j]=0.\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise: }$ Using the above Matern 5/2 and 3/2 examples as a template write a python method for the general Matern kernel. Can you think of some real world examples where you might a priori know to use each of these kernels?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Wendland Polynomial Covariance\n",
    "\n",
    "ZOHEYR. PLEASE WRITE SOMETHING SIMILAR TO SECTION 3.3.1 and 3.3.2 HERE, BUT THIS TIME FOR THE WENDLAND POLYNOMIALS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Building New Covariance Functions From Old Ones\n",
    "\n",
    "Given one or more \"seed\" covariance functions there are several useful ways to construct new covariance functions. \n",
    "\n",
    "$\\textbf{Summation: }$ For any two covariance functions, $k_{1}(x,x')$ and $k_{2}(x,x')$, the sum $k(x,x')=k_{1}(x,x')+k_{2}(x,x')$ is a new covariance function.\n",
    "\n",
    "$\\textbf{Product: }$ For any two covariance functions, $k_{1}(x,x')$ and $k_{2}(x,x')$, the product $k(x,x')=k_{1}(x,x')\\times k_{2}(x,x')$ is a new covariance function.\n",
    "\n",
    "$\\textbf{Warping: }$ For any covariance function $k(x,x')$, and any \"warping\" function $u:\\mathcal{S}\\rightarrow\\mathcal{S}$, the function $k(\\,u(x)\\,,\\,u(x')\\,)$ is a new covariance function.\n",
    "\n",
    "$\\textbf{Renormalisation: }$ For any covariance function $k(x,x')$, and any function $\\alpha:\\mathcal{S}\\rightarrow\\mathbb{R}$, the combination $\\alpha(x)k(x,x')\\alpha(x')$ is a new covariance function.\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "$\\textbf{Exercise: }$ Prove that all of the above procedures do indeed generate new symmetric positive semi-definite covariance functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Mean Square Continuity and Differentiability\n",
    "\n",
    "In this section it will be shown how certain properties of the covariance function are responsible for governing the continuity and differentiability of the Gaussian process (GP). \n",
    "\n",
    "Here, the notion $\\textbf{mean square continuity}$ is used (defined below in equation (3.1)); other defintions of GP continuity exists, but the notion used here relates can be most easily related to the covariance function. For more mathematical details on the topics discussed in this section see [ADLER].\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "Consider a Gaussian process, $f$, on the set $\\mathcal{S}=\\mathbb{R}^{n}$;\n",
    "\n",
    "$$f(x) \\sim \\mathcal{GP}( \\, \\mu(x) \\, , \\, k(x,x') \\, ) \\,. $$\n",
    "\n",
    "The smoothness and differentiability properties of $f$ at some particular point $x_{*}\\in\\mathbb{R}^{n}$ will be considered.\n",
    "Let $x_{1},x_{2},x_{3}\\ldots\\in\\mathbb{R}^{n}$ be a sequence of points which converge to $x_{*}$ in the sense that $\\lim_{\\ell\\rightarrow\\infty}\\left|x_{\\ell}-x_{*}\\right|=0$, where $|\\cdot|$ denotes the usual Euclidean norm.\n",
    "\n",
    "The process $f(x)$ is said to be mean square continuous at $x_{*}$ if\n",
    "\n",
    "$$\\lim_{\\ell\\rightarrow\\infty}\\,\\mathrm{E}\\Big[\\big(f(x_{*})-f(x_{\\ell})\\big)^{2}\\Big]=0 \\, , \\hspace{2cm} \\textrm{(3.1)}$$ \n",
    "\n",
    "and the mean square limit, if it exists, is denoted as\n",
    "\n",
    "$$ f(x_{*}) = \\mathrm{l.i.m.}_{\\ell\\rightarrow\\infty}\\,f(x_{\\ell}) \\,, $$\n",
    "\n",
    "where $\\mathrm{l.i.m.}$ stands for limit in mean square.\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "It should be noted that mean square continuity automatically implies continuity of the mean,\n",
    "\n",
    "$$ \\lim_{\\ell\\rightarrow\\infty}\\,\\mathrm{E}\\big[f(x_{\\ell})-f(x_{*})\\big] = 0 \\,.  \\hspace{2cm} \\textrm{(3.2)}$$\n",
    "\n",
    "$\\textbf{Exercise: }$ Prove the result for the continuity of the mean stated in equation (3.2).\n",
    "\n",
    "$\\textbf{Hint: }$ Consider the variance of the quantity $\\big(f(x_{\\ell})-f(x_{*})\\big)$.\n",
    "\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "It will now be shown how the mean square continuity of the GP can be related to properties of the covariance function.\n",
    "\n",
    "$\\textbf{Lemma: }$ If the random process $f$ is continuous in mean square at $x_{∗}$ then covariance function $k(x,x′)$ is continuous in the usual sense at the point $x=x′=x_{∗}$. \n",
    "\n",
    "$\\textbf{Proof: }$ The mean and covariance of the process $f$ are, by definition, given by\n",
    "\n",
    "$$ \\mu(x) = \\mathrm{E}\\big[f(x)\\big] \\,, $$\n",
    "$$ k(x,x') = \\mathrm{E}\\Big[\\big(f(x)-\\mu(x)\\big)\\big(f(x')-\\mu(x')\\big)\\Big] \\,. $$\n",
    "\n",
    "The condition for mean square continuity is given in equation (3.1), this may be rewritten as\n",
    "\n",
    "$$ \\lim_{\\ell\\rightarrow\\infty}\\,\\Big( k(x_{*},x_{*})-2k(x_{\\ell},x_{*})+k(x_{\\ell},x_{\\ell}) + \\big(\\mu(x_{*})-\\mu(x_{\\ell})\\big)^{2} \\Big) = 0 \\,. $$\n",
    "\n",
    "(It is relativiely easy to check this by using the defitions for $\\mu(x)$ and $k(x,x')$ and expanding everything out.)\n",
    "\n",
    "If the process $f$ is mean square continous then it is also continuous in the mean; using the continuity of the mean gives\n",
    "\n",
    "$$ \\lim_{\\ell\\rightarrow\\infty}\\, \\big( k(x_{*},x_{*}) -2k(x_{*},x_{\\ell}) + k(x_{\\ell},x_{\\ell})\\big) = 0 \\,. $$\n",
    "\n",
    "This is satisfied if the covariance function is continuous in the usual sense at the point $x=x'=x_{*}$.\n",
    "\n",
    "$\\textbf{Comments: }$ In fact, a GP is continuous in mean square if $\\textbf{and only if}$ the covariance function is continuous at $x=x′=x_{∗}$, although this is not proved here.\n",
    "For a stationary covariance function (i.e. $k(x,x')=k(\\tau)$, where $\\tau=x-x'$) checking if this condition holds reduces to simply checking the continuity of $k(\\tau)$ at the point $\\tau=0$.\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "We now move on from considering continuity to consider differentiability. \n",
    "In the spirit of the above the notion of taking the mean square derivative of a GP is defined as\n",
    "\n",
    "$$ \\frac{D f(x)}{D x^{i}} = \\mathrm{l.i.m.}_{\\epsilon\\rightarrow 0} \\frac{f(x_{*}+\\epsilon \\mathbf{e}_{i})-f(x_{*})}{\\epsilon} $$\n",
    "\n",
    "where $\\mathbf{e}_{i}$ is the unit vector in the $i$-direction, and the $\\textrm{l.i.m.}$ is the limit in mean square. \n",
    "This definition can be easily extended to higher-order derivatives. \n",
    "The mean square derivative of a GP is also a GP; this is a consequence of the fact that the sum of two Gaussian random variables is also distributed as a Gaussian random variable.\n",
    "\n",
    "$\\textbf{Lemma: }$ The covariance function of the GP $\\frac{D f(x)}{D x_{i}}$ is given by $\\frac{\\partial^{2}k(x,x′)}{\\partial x_{i1}\\partial x′_{i1}\\partial x_{i2}\\partial x′_{i2}\\ldots \\partial x_{in}\\partial x′_{in}}$. \n",
    "\n",
    "$\\textbf{Proof: }$\n",
    "\n",
    "$\\textbf{Comments: }$ By extension, the covariance function of the GP $\\frac{D^{n} f(x)}{D x_{i1} D x_{i1} \\ldots D x_{in}}$ is given by $\\frac{\\partial^{2n}k(x,x′)}{\\partial x_{i}\\partial x′_{i}}$. \n",
    "From the above results relating the mean square continuity of GPs to the continuity of the covariance function at $x=x'=x_{*}$, it follows that the $n^{\\mathrm{th}}$ mean square derivative of the GP is mean square continuous (i.e. the GP is $n$-times mean square differentiable) if the $2n^{\\mathrm{th}}$ regular derivative of the covariance function is continuous at $x=x'=x_{*}$. Notice that it is again the smoothness properties of the covariance function along the diagonal points that determines the differentiability of the GP. It can also be shown that if a covariance function is continuous at all diagonal points $x=x'$ then it’s everywhere continuous. (For a stationary GP it is sufficient to check the continuity of $k(\\tau)$ at $\\tau=0$.)\n",
    "\n",
    "$$\\phantom{.}$$\n",
    "\n",
    "$\\textbf{Exercise: }$ Use these results to establish the level of mean square differentiability of GPs with the squared exponential Matern kernels. Go back to section (?) and rerun the code cells to generate more realisations of the Matern GP with different values of $\\nu$ and observe the smoothness properties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Gaussian Processes Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Regression with Gaussian Processes\n",
    "\n",
    "In the examples presented thus far, random samples have been generated from some covariance matrices that describes the full joint distribution on function values.  In Gaussian process regression, one is interested in the _conditional_ distributions of function values.  We seek to infer possible (unknown) function values conditioned on some known values of the function. We can write the joint probability distribution of unknown function values ${\\bf f}_*$ at some target points $X_*$ and the known values ${\\bf f}$ at some sample points $X$ as:\n",
    "\n",
    "$$p([{\\bf f}, {\\bf f}_*]) = \\frac{1}{(2\\pi)^{n/2}|K|^{1/2}}\\exp\\left(-\\frac{1}{2}[{\\bf f}, {\\bf f}_*]^\\top K^{-1}[{\\bf f}, {\\bf f}_*]\\right) \\hspace{2cm} \\textrm{(4.1)}$$\n",
    "\n",
    "Let's define:\n",
    "$$\\Gamma = K^{-1} = \n",
    "\\begin{pmatrix} \n",
    "\\Gamma_{XX} & \\Gamma_{XX_*}  \\\\ \n",
    "\\Gamma_{XX_*} & \\Gamma_{X_*X_*}  \\\\\n",
    "\\end{pmatrix}, \n",
    "K=\n",
    "\\begin{pmatrix} \n",
    "K_{XX} & K_{XX_*}  \\\\ \n",
    "K_{X_*X} & K_{X_*X_*}  \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "If we condition on known ${\\bf f}$, then all terms with only ${\\bf f}$'s become constants and can be ignored:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p({\\bf f}_*|{\\bf f}) & \\propto \\exp\\left(-\\frac{1}{2} {\\bf f_*}^\\top\\Gamma_{X_*X_*}{\\bf f_*} - {\\bf f}^\\top\\Gamma_{XX_*}{\\bf f_*}\\right) \\hspace{2cm} \\textrm{(4.2)}\\\\\n",
    "& \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "The relations between the submatrices of $\\Gamma$ and $K$ are:\n",
    "\n",
    "$$\\Gamma_{XX_*} = -K_{XX}^{-1}K_{XX_*}\\left(K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}\\right)^{-1}$$\n",
    "$$\\Gamma_{X_*X_*} = \\left(K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}\\right)^{-1}$$\n",
    "\n",
    "The $K$ matrices are the covariances between test and/or input values of $f(x)$.  Substituting those into Equation (4.2) yields:\n",
    "\n",
    "$$p({\\bf f}_*|{\\bf f})  \\propto \\exp\\left(-\\frac{1}{2} {\\bf f_*}^\\top\\left(K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}\\right)^{-1}{\\bf f_*} - {\\bf f}^\\top\\left(-K_{XX}^{-1}K_{XX_*}\\left(K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}\\right)^{-1}\\right){\\bf f_*}\\right)$$\n",
    "\n",
    "We can complete the square by adding this term in the exponential:\n",
    "\n",
    "$$-\\frac{1}{2}{\\bf f}^\\top\\left(K_{XX}^{-1}K_{XX_*}\\left(K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}\\right)^{-1}K_{X_*X}K_{XX}^{-1}\\right){\\bf f}$$\n",
    "\n",
    "Notice that this term is independent of ${\\bf f}_*$, so adding it in the exponential only affects the normalization.  After completing the square, we get \n",
    "\n",
    "$$p({\\bf f}_*|{\\bf f})  \\propto \\exp\\left(-\\frac{1}{2}({\\bf f}_* - K_{X_*X}K_{XX}^{-1}{\\bf f})^\\top\\left(K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}\\right)^{-1}({\\bf f}_* - K_{X_*X}K_{XX}^{-1}{\\bf f})\\right)$$\n",
    "\n",
    "We see that this is now a multivariate Gaussian with mean $K_{X_*X}K_{XX}^{-1}{\\bf f}$ and covariance $K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}$.  This result is the workhorse of Gaussian process regression, and we summarize it here:\n",
    "\n",
    "$$ p({\\bf f}_*|{\\bf f}) \\sim \\mathcal{N}\\left(K_{X_*X}K_{XX}^{-1}{\\bf f}, \\hspace{0.2cm} K_{X_*X_*}-K_{X_*X}K_{XX}^{-1}K_{XX_*}\\right) \\hspace{2cm} \\textrm{(4.3)}$$\n",
    "\n",
    "Let's take a second to digest this equation.  First, we can see that the mean values of ${\\bf f}_*$ are generated by a linear transformation of the input values ${\\bf f}$. Second, the variance of the conditional distribution is independent of ${\\bf f}$ -- it only depends on the $K$ matrices, which are constructed only from $X$, $X_*$ and the kernel.  Therefore, local behavior of the values of ${\\bf f}$ does not affect the predicted conditional variances. Also note that the first term of the conditional covariance is simply the prior covariance $K_{X_*X_*}$ and from it is subtracted $K_{X_*X}K_{XX}^{-1}K_{XX_*}$.  This second term is large when values of $K_{X_*X}$ and $K_{XX_*}$ are large and/or $K_{XX}$ values are small. Thus if a kernel such as the squared-exponential is used, where the covariance falls off with the distance between parameter values, ${\\bf f}_*$ will be distributed like the prior if $X_*$ are \"far away\" from $X$ since the second term will go to zero.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Computing a GPR\n",
    "\n",
    "Now that we have Equation 4.3, let's perform some regression!  To do this, we'll need to choose on of the covariance functions described in section 3.3 and we'll require a bit more machinery in terms of practically performing the matrix operations required on a computer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's condition the GP we created with our squared-exponential kernel.  To do this, we need to take the inverse of $K_{XX}$.  We will effectively do this via the Cholesky decomposition, which decomposes a matrix $K$ as $K=LL^\\top$, where $L$ and $L^\\top$ are lower and upper triangular matrices, respectively. Rather than directly find the inverse of $K$, we can solve for the vector ${\\bf x}$ which fulfills the equation:\n",
    "$$ {\\bf y} = K{\\bf x} = LL^\\top{\\bf x}$$\n",
    "We write the solution as ${\\bf x} = K\\setminus {\\bf y}$.  Substituting in the triangular matrices, we have ${\\bf x} = L^\\top\\setminus (L \\setminus {\\bf y})$.  To solve for ${\\bf b} = L \\setminus {\\bf y}$, use forward substitution and for $x = L^\\top\\setminus {\\bf b}$ use backward substitution. With this inversion scheme in hand, we can write the numerical procedure algorithmically as (following Rasmussen and Williams Algorithm 2.1):\n",
    "\n",
    "1. $L := {\\rm Cholesky}(K)$\n",
    "2. ${\\bf \\alpha} := L^\\top \\setminus (L \\setminus {\\bf f})$\n",
    "3. ${\\bf \\bar{f}_*} := {\\bf k}_*^\\top {\\bf \\alpha}$\n",
    "4. ${\\bf v} := L \\setminus {\\bf k}_*$\n",
    "5. $\\mathbb{V} := {\\bf k}_* - {\\bf v}^\\top {\\bf v}$\n",
    "6. $\\log p({\\bf f}|X) := -\\frac{1}{2}{\\bf f }^\\top {\\bf \\alpha} - \\sum_i L_{ii} - \\frac{n}{2}\\log(2\\pi)$\n",
    "\n",
    "Here, ${\\bf k}_*:= k({\\bf x_*},X)$ is the kernel function $k$ evaluated at the test point ${\\bf x_*}$ and the set of input points $X$. If one is only interested in the mean ${\\bf \\bar{f}_*}$, then one only need save out ${\\bf \\alpha}$ as the trained interpolator.  If variances are desired (which is typically the point of using GPR), $L$ must also be saved.  Line 6 of the algorithm computes the log marginal likelihood of values ${\\bf f}$ under the Gaussian process prior -- it will come in handy later.  In the cell below, we follow the algorithm above to condition a GP on a few known values of the sine function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood is -23.509304196808344\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEICAYAAAC55kg0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181c9cd550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The training points and training values\n",
    "X = np.linspace(1,10,20).reshape(-1,1)\n",
    "ferr = 1e-1\n",
    "f = np.sin(4.*X)#+np.random.normal(0.,ferr,size=X.shape)\n",
    "\n",
    "# The test points and real values of f at test points\n",
    "Xstar = np.linspace(0,11,300).reshape(-1,1)\n",
    "fstar = np.sin(4.*Xstar)\n",
    "\n",
    "# Choose a kernel, nugget, and compute covariance matrices\n",
    "length_scale = .05\n",
    "cov_scale = 1.\n",
    "KXX = cov_scale*squared_exponential_kernel(X,X,scale=length_scale,metric=None)\n",
    "KXXstar = cov_scale*squared_exponential_kernel(Xstar,X,scale=length_scale,metric=None)\n",
    "\n",
    "nugget_val = 1e-2\n",
    "nugget = np.diag((nugget_val**2)*np.ones(X.shape[0]))\n",
    "\n",
    "# Steps 1-2\n",
    "L = LA.cholesky(KXX+nugget,lower=True)\n",
    "alpha = LA.cho_solve((L,True), f)\n",
    "\n",
    "# Steps 3-5\n",
    "fstar_bar = np.dot(KXXstar,alpha)\n",
    "variances = []\n",
    "for i in range(KXXstar.shape[0]):\n",
    "    v = LA.solve_triangular(L,KXXstar[i,:],lower=True)\n",
    "    variances.append(cov_scale-np.dot(v.T,v))\n",
    "stds = np.sqrt(variances)\n",
    "\n",
    "# Step 6\n",
    "lml = -(1./2.)*np.dot(f.T,alpha) - np.sum(np.log(np.diag(L))) - (X.shape[0]/2.)*np.log(2*np.pi)\n",
    "lml = lml.flatten()[0]\n",
    "print ( \"The log likelihood is\" , lml )\n",
    "\n",
    "plt.plot(X,f,'ro')\n",
    "plt.plot(Xstar,fstar_bar,label='GP mean')\n",
    "plt.plot(Xstar,fstar,label='sin(4x)')\n",
    "\n",
    "Xstar_flat,fstar_bar_flat=Xstar.flatten(),fstar_bar.flatten()\n",
    "plt.fill_between(Xstar_flat,fstar_bar_flat-stds,fstar_bar_flat+stds,alpha=0.5,color='gray')\n",
    "plt.ylim([-5,5])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Choosing the Kernel and Hyperparameters:\n",
    "\n",
    "The kernel and it's associated hyperparameters are the bedrock of the Gaussian process regression and determine the overall morphology of the interpolations.  In most cases of interpolating smooth functions, the squared-exponential will suffice, and in simple cases one can even hand tune the hyperparameters.  In the machine-learning context though, one is more interested in fitting the hyperparameters to maximize some objective function of the observed data.  The squared-exponential kernel may not be right for all cases, either, as we shall see.  \n",
    "\n",
    "The most natural objective function for deciding on kernel and/or hyperparameter choice is the log marginal likelihood, i.e. the probability of the observed data given the kernel/hyperparameter choice $p(f|k,\\theta)$.  I call this the \"hyperlikelihood\". Let's perform the same interpolation as above but choose hyperparameters that maximize this.  One can also optionally introduce a \"hyperprior\" $p(\\theta)$, ie. a prior distribution on hyperparameter values. This can be helpful if the marginal likelihood is not well behaved or has many local minima.  The \"hyperposterior\" $p(\\theta|f)$ is given by:\n",
    "\n",
    "$$ p(\\theta,k|f) \\propto p(f|k,\\theta)p(\\theta,k) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='raise',under='warn',over='raise')\n",
    "def log_marginal_likelihood(theta,X,f,sign=1,cholesky=True,kernel='SE'):\n",
    "    \"\"\"\n",
    "    Compute the log_marginal_likelihood of data f\n",
    "    at points X under GP given by covariance K\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: array\n",
    "        array of hyperparameters 10**[nugget_val,cov_scale,length_scale]\n",
    "    X: array\n",
    "        array of GP input points\n",
    "    f: array\n",
    "        array of data at X\n",
    "    sign: float\n",
    "        A multiplier that is meant to be 1 or -1 \n",
    "        depending on if one wants the negative or \n",
    "        positive log marginal likelihood\n",
    "    Returns\n",
    "    -------\n",
    "    lml: float\n",
    "        log marginal likelihood of data f\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cov_scale = 10.**theta[0]\n",
    "        length_scale = 10.**theta[1]\n",
    "    except FloatingPointError:\n",
    "        return -np.inf*sign\n",
    "    try:\n",
    "        if kernel == 'SE':\n",
    "            K = cov_scale*squared_exponential_kernel(X,X,scale=length_scale,metric=None)\n",
    "        elif kernel == 'Matern32':\n",
    "            K = cov_scale*Matern32(X,X,scale=length_scale,metric=None)\n",
    "        elif kernel == 'Matern52':\n",
    "            K = cov_scale*Matern52(X,X,scale=length_scale,metric=None)\n",
    "        if np.any(np.isnan(K)):\n",
    "            print ( 'K has nans, returning nll = inf' )\n",
    "            print ( theta )\n",
    "            return -np.inf*sign\n",
    "        nugget = np.diag((10.**theta[2])*np.ones(X.shape[0]))\n",
    "        if cholesky:\n",
    "            L = spl.cholesky(K+nugget,lower=True)\n",
    "            alpha = spl.cho_solve((L,True), f)\n",
    "            lml = -(1./2.)*np.dot(f.T,alpha) - np.sum(np.log(np.diag(L))) - (X.shape[0]/2.)*np.log(2*np.pi)\n",
    "            lml = lml.flatten()[0]\n",
    "        else:\n",
    "            Kinv = spl.inv(KXX+nugget)\n",
    "            alpha = np.dot(Kinv,f)\n",
    "            # Steps 3-5\n",
    "            variances = []\n",
    "            for i in range(KXXstar.shape[0]):\n",
    "                v = np.dot(KXXstar[i,:].T,np.dot(Kinv,KXXstar[i,:]))\n",
    "                variances.append(v)\n",
    "            stds = np.sqrt(variances)\n",
    "            # Step 6\n",
    "            lml = -(1./2.)*np.dot(f.T,alpha) - 0.5*np.log(np.linalg.det(Kinv)) - (X.shape[0]/2.)*np.log(2*np.pi)\n",
    "            lml = lml.flatten()[0]\n",
    "    except:\n",
    "        return -np.inf*sign\n",
    "    return lml*sign\n",
    "\n",
    "\n",
    "# def get_dK_dtheta(theta,X):\n",
    "#     \"\"\"\n",
    "#     Compute dK/dtheta\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     theta: array\n",
    "#         list of hyperparameters\n",
    "#     \"\"\"\n",
    "#     dK_dtheta = []\n",
    "#     #npoints = X.shape[0]\n",
    "#     #ndim = X.shape[1]\n",
    "#     cov_scale = 10.**theta[0]\n",
    "#     scale = 10.**theta[1]\n",
    "#     cov_scale = theta[0]\n",
    "#     scale = theta[1]\n",
    "#     if isinstance(scale,(float,int)):\n",
    "#         inv_scale=(1./scale)*np.ones(ndim)\n",
    "#         inv_scale = np.diag(inv_scale)\n",
    "#     else:\n",
    "#         inv_scale = 1./scale\n",
    "#         inv_scale = np.diag(inv_scale)\n",
    "#     dist = lambda a,b: np.sqrt(np.dot(np.dot(a-b,inv_scale),a-b))\n",
    "#     for n in range(len(theta)): \n",
    "#         if n==0:\n",
    "#             dK_dtheta_n = squared_exponential_kernel(X,X,scale=scale,metric=None)\n",
    "#         else:\n",
    "#             dK_dtheta_n = np.empty([npoints,npoints])\n",
    "#             for i in range(npoints):\n",
    "#                 for j in range(npoints):\n",
    "#                     dK_dtheta_n[i,j] = -0.5*cov_scale*(((X[i,:]-X[j,:])**2)/scale)\n",
    "#             dK_dtheta_n = np.multiply(dK_dtheta_n,squared_exponential_kernel(X,X,scale=scale,metric=None))\n",
    "#         dK_dtheta.append(dK_dtheta_n)\n",
    "#     return dK_dtheta\n",
    "\n",
    "# def get_dLogP_dtheta_SE(theta,X,f,sign=1):\n",
    "#     \"\"\"\n",
    "#     Compute the gradient of the log\n",
    "#     likelihood for the squared exponential\n",
    "#     as a function of hyperparameters\n",
    "#     \"\"\"\n",
    "#     cov_scale = 10.**theta[0]\n",
    "#     length_scale = 10.**theta[1]\n",
    "#     #cov_scale = theta[0]\n",
    "#     #length_scale = theta[1]\n",
    "#     npoints = X.shape[0]\n",
    "#     nhyperparam = len(theta)\n",
    "#     K = cov_scale*squared_exponential_kernel(X,X,scale=length_scale,metric=None)\n",
    "#     Kinv = spl.inv(K)\n",
    "#     alpha = np.dot(Kinv,f)\n",
    "#     dK_dtheta = get_dK_dtheta(theta,X)\n",
    "#     Kinvf = np.dot(Kinv,f)\n",
    "#     dLogP_dtheta_SE = []\n",
    "#     for n in range(nhyperparam):\n",
    "#         dLogP_dtheta_SE_n = 0.5*(np.dot(np.dot(Kinvf.T,dK_dtheta[n]),Kinvf) - np.trace(np.dot(Kinv,dK_dtheta[n])))\n",
    "#         dLogP_dtheta_SE.append(dLogP_dtheta_SE_n[0,0])\n",
    "\n",
    "#     return np.array(dLogP_dtheta_SE)*sign\n",
    "        \n",
    "from scipy.stats import norm\n",
    "def log_hyperprior(theta,theta_means,theta_widths,prior_type='normal'):\n",
    "    \"\"\"\n",
    "    The log hyperprior\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: array\n",
    "        array of parameter values\n",
    "    theta_means: array\n",
    "        array of mean values of theta\n",
    "    theta_widths: array\n",
    "        gaussian widths of theta values\n",
    "    prior_type: string\n",
    "        either normal or box\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_h: float\n",
    "        log hyperprior\n",
    "    \"\"\"\n",
    "    log_h = 0.\n",
    "    for i in range(len(theta)):\n",
    "        try:\n",
    "            if prior_type=='normal':\n",
    "                log_h = log_h + np.log(norm.pdf(theta[i], theta_means[i], theta_widths[i]))\n",
    "            else:\n",
    "                if ~((theta[i]>(theta_means[i]-theta_widths[i])) and \n",
    "                    (theta[i]<(theta_means[i]+theta_widths[i]))):\n",
    "                    log_h = -np.inf\n",
    "                else:\n",
    "                     pass                                                     \n",
    "        except:\n",
    "            -np.inf\n",
    "    return log_h\n",
    "\n",
    "def log_hyperposterior(theta,X,f,\n",
    "                       theta_means,theta_widths,\n",
    "                       sign=1,cholesky=True,\n",
    "                       kernel='SE',prior_type='normal'):\n",
    "    \"\"\"\n",
    "    The log hyperposterior\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta: array\n",
    "        array of hyperparameters\n",
    "    X: array\n",
    "        array of GP input points\n",
    "    f: array\n",
    "        array of data at X\n",
    "    theta_means: array\n",
    "        array of mean values of lognormal prior\n",
    "    theta_widths: array\n",
    "        widths of lognormal prior\n",
    "    sign: float\n",
    "        1 or -1 depending on if negative \n",
    "        log hyperposterior is wanted\n",
    "    Returns\n",
    "    -------\n",
    "    log_hyperposterior: float\n",
    "        the log hyperposterior\n",
    "    \"\"\"\n",
    "    hyperpos = (log_marginal_likelihood(theta,X,f,cholesky=cholesky,kernel=kernel)+\n",
    "                log_hyperprior(theta,theta_means,theta_widths,prior_type))\n",
    "    if np.isnan(hyperpos):\n",
    "        return -np.inf*sign\n",
    "    else:\n",
    "        return hyperpos*sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a look at the hyperlikelihood or hyperposterior as a function of the hyperparameters, you can use an mcmc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermoore/anaconda3/lib/python3.6/site-packages/emcee/ensemble.py:335: RuntimeWarning: invalid value encountered in subtract\n",
      "  lnpdiff = (self.dim - 1.) * np.log(zz) + newlnprob - lnprob0\n",
      "/Users/christophermoore/anaconda3/lib/python3.6/site-packages/emcee/ensemble.py:336: RuntimeWarning: invalid value encountered in greater\n",
      "  accept = (lnpdiff > np.log(self._random.rand(len(lnpdiff))))\n",
      "WARNING:root:Too few points to create valid contours\n",
      "WARNING:root:Too few points to create valid contours\n",
      "WARNING:root:Too few points to create valid contours\n"
     ]
    }
   ],
   "source": [
    "ndim, nwalkers = 3, 20\n",
    "theta_means = [0.0,0.0,-3]\n",
    "theta_widths = [2.,3.,3.] # order of mag width on covariance and length, 0.1 orders on nugget\n",
    "sampler = mc.EnsembleSampler(nwalkers, ndim, log_hyperposterior, args=(X, f,theta_means,theta_widths,1,True,'SE','box'))\n",
    "\n",
    "\n",
    "pos = [[0.,0.,-3.] + 0.1*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "sampler.run_mcmc(pos, 500);\n",
    "import corner\n",
    "samples = sampler.chain[:, 200:, :].reshape((-1, ndim))\n",
    "corner.corner(samples, labels=[\"$\\log_{10}\\sigma^2$\", \"$\\log_{10} l$\",\"$\\log_{10}$ nugget\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are multiple local extrema to the objective functions we are trying to maximize/minimize.  As a result, some tweaking or multiple optimizer restarts may be required if we want to just use the hyperparameters at extrema of the objective function.  One could in principle take an even more Bayesian approach and marginalize over hyperparameters rather than choosing one set of hyperparameters.  This would be total overkill in most applications. It's also worth noting that the hyperposterior's profile is telling us about different ways to explain the data (duh, it's Bayes!).  In this case with the sinusoidal data, it could be that all the data could be explained as mostly noise or it could instead be that there is a small amount of noise in the data and that $\\sigma^2 \\sim 10^{-0.4}$ and $l \\sim 10^{-2}$.  In some cases you may know the noise level, in which case it can be fixed and the other hyperparameters can be optimized.  Let's fix the nugget to 1e-2 and see how the other hyperparameters are optimized:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermoore/anaconda3/lib/python3.6/site-packages/emcee/ensemble.py:335: RuntimeWarning: invalid value encountered in subtract\n",
      "  lnpdiff = (self.dim - 1.) * np.log(zz) + newlnprob - lnprob0\n",
      "/Users/christophermoore/anaconda3/lib/python3.6/site-packages/emcee/ensemble.py:336: RuntimeWarning: invalid value encountered in greater\n",
      "  accept = (lnpdiff > np.log(self._random.rand(len(lnpdiff))))\n",
      "WARNING:root:Too few points to create valid contours\n",
      "WARNING:root:Too few points to create valid contours\n",
      "WARNING:root:Too few points to create valid contours\n"
     ]
    }
   ],
   "source": [
    "ndim, nwalkers = 3, 20\n",
    "theta_means = [0.0,0.0,-2]\n",
    "theta_widths = [2.,3.,0.01] # order of mag width on covariance and length, 0.1 orders on nugget\n",
    "sampler = mc.EnsembleSampler(nwalkers, ndim, log_hyperposterior, args=(X, f,theta_means,theta_widths,1,True,'SE','box'))\n",
    "\n",
    "pos = [[0.,0.,-2.] + 0.1*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "sampler.run_mcmc(pos, 500);\n",
    "import corner\n",
    "samples = sampler.chain[:, 200:, :].reshape((-1, ndim))\n",
    "corner.corner(samples, labels=[\"$\\log_{10}\\sigma^2$\", \"$\\log_{10} l$\",\"$\\log_{10}$ nugget\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's minimize the negative log marginal likelihood for data f assuming a \"nugget\" of 1e-2.  In order to make the minimization easy, we've used the above corner plots to inform the hyperprior to carve out the area of parameter space where we want the minimizer to search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermoore/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: underflow encountered in exp\n",
      "/Users/christophermoore/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: underflow encountered in multiply\n",
      "/Users/christophermoore/anaconda3/lib/python3.6/site-packages/scipy/optimize/optimize.py:628: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "K has nans, returning nll = inf\n",
      "[nan nan nan]\n",
      "      fun: inf\n",
      " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([nan, nan, nan])\n",
      "  message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
      "     nfev: 84\n",
      "      nit: 0\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([-0.25, -2.  , -2.  ])\n"
     ]
    }
   ],
   "source": [
    "minres = None\n",
    "minresval = np.inf\n",
    "bounds = ([-0.5,0.0],[-3,-1],[-2.1,-1.92])\n",
    "x0 = [-0.25,-2.,-2.]\n",
    "res = spo.minimize(log_hyperposterior,\n",
    "                   x0=x0,\n",
    "                   args=(X,f,theta_means,theta_widths,-1.,True,'SE','normal'),\n",
    "                   bounds = bounds,\n",
    "                  )\n",
    "print ( res )\n",
    "# theta_widths = [2.,3.,0.1]\n",
    "# for i in range(200):\n",
    "#     sel = False\n",
    "#     while ~sel:\n",
    "#         x0 = [0.0,0.0,-2]+0.2*np.multiply(np.random.randn(3),[1.,1.,0.01]) \n",
    "#         sel = True\n",
    "#         for j in range(len(x0)):\n",
    "#             sel = sel and (x0[j]<bounds[j][1]) and (x0[j]>bounds[j][0])\n",
    "#     res = spo.minimize(log_hyperposterior,\n",
    "#                        x0=x0,\n",
    "#                        args=(X,f,theta_means,theta_widths,-1.,True,'SE','normal'),\n",
    "#                        bounds = bounds,\n",
    "#                       )\n",
    "#     if ((res[\"fun\"]<minresval) and (res[\"fun\"]!=(-np.inf))):\n",
    "#         minres = res\n",
    "\n",
    "# print minres\n",
    "#res = spo.minimize(log_marginal_likelihood,x0=[1.,1.],args=(X,f),jac=get_dLogP_dtheta_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use optimized hyperparameters\n",
    "cov_scale = 10**res[\"x\"][0]\n",
    "length_scale = 10**res[\"x\"][1]\n",
    "nugget_val = 10**res[\"x\"][2]\n",
    "\n",
    "nugget = np.diag(nugget_val*np.ones(X.shape[0]))\n",
    "# avoid underflow errors by working in log space:\n",
    "KXX = cov_scale*squared_exponential_kernel(X,X,scale=length_scale,metric=None)\n",
    "KXXstar = cov_scale*squared_exponential_kernel(Xstar,X,scale=length_scale,metric=None)\n",
    "\n",
    "# Steps 1-2\n",
    "L = spl.cholesky(KXX+nugget,lower=True)\n",
    "alpha = spl.cho_solve((L,True), f)\n",
    "\n",
    "# Steps 3-5\n",
    "fstar_bar = np.dot(KXXstar,alpha)\n",
    "variances = []\n",
    "for i in range(KXXstar.shape[0]):\n",
    "    v = spl.solve_triangular(L,KXXstar[i,:],lower=True)\n",
    "    variances.append(cov_scale-np.dot(v.T,v))\n",
    "stds = np.sqrt(variances)\n",
    "\n",
    "# Step 6\n",
    "lml = -(1./2.)*np.dot(f.T,alpha) - np.sum(np.log(np.diag(L))) - (1./2.)*np.log(2*np.pi)\n",
    "lml = lml.flatten()[0]\n",
    "print 'The log likelihood is',lml\n",
    "\n",
    "plt.plot(X,f,'ro')\n",
    "plt.plot(Xstar,fstar_bar,label='GP mean')\n",
    "plt.plot(Xstar,fstar,label='sin(4x)')\n",
    "\n",
    "Xstar_flat,fstar_bar_flat=Xstar.flatten(),fstar_bar.flatten()\n",
    "plt.fill_between(Xstar_flat,fstar_bar_flat-stds,fstar_bar_flat+stds,alpha=0.5,color='gray')\n",
    "plt.ylim([-5,5])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(Xstar,np.divide(np.abs(fstar-fstar_bar).flatten(),stds),label='mean residual')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(r'$\\frac{f-\\overline{f}}{\\delta f_{GP}}$',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now finished running a trained Gaussian Process Regression!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Gaussian Process Regression: A Gravitational Wave Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a gravitational-wave example!  Rather than interpolate full waveforms, which is beyond what we have time for in this tutorial, we will instead interpolate fraction of mass loss from binary mergers as a function of mass ratio.  At our disposal, we have SXS simulations of black hole mergers.  Some outputs of these simulations include radiated energies.  Let's first get the data and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadMetaData ( ID ):\n",
    "    \n",
    "    fname = \"GPR_Chris_Zoheyr/data/SXS_data/\" + ID + \"/metadata\" + ID.split('-')[1] + \".txt\"\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    for line in content:\n",
    "        if \"relaxed-mass1\" in line:\n",
    "            relaxed_mass1 = float ( line.split()[-1] )\n",
    "        elif \"relaxed-mass2\" in line:\n",
    "            relaxed_mass2 = float ( line.split()[-1] )\n",
    "        elif \"remnant-mass\" in line:\n",
    "            remnant_mass = float ( line.split()[-1] )\n",
    "    \n",
    "    total_mass = relaxed_mass1 + relaxed_mass2\n",
    "    mass_ratio = relaxed_mass1 / relaxed_mass2\n",
    "    Delta_E = ( total_mass - remnant_mass ) / total_mass\n",
    "    \n",
    "    return np.array ( [ mass_ratio , Delta_E ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "ids = os.listdir ( \"GPR_Chris_Zoheyr/data/SXS_data/\" )\n",
    "mass_ratios = []\n",
    "energies = []\n",
    "for idval in ids:\n",
    "    mass_ratio, Delta_E = LoadMetaData ( idval )\n",
    "    mass_ratios.append(mass_ratio)\n",
    "    energies.append(Delta_E)\n",
    "mass_ratios,energies = np.array(mass_ratios),np.array(energies)\n",
    "plt.scatter(mass_ratios,energies)\n",
    "plt.xlabel('Mass Ratio')\n",
    "plt.ylabel('1- remnant mass / initial mass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the remnant mass over the initial mass goes to zero as the mass ratio is increased. This makes sense in the high mass ratio limit where one would expect no energy radiated from throwing a teeny black hole into a super-massive one.  Let's start by breaking the data set into a training set and a validation set.  We'll use 8 of the 19 simulations to train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "indices = np.random.choice(range(len(mass_ratios)),replace=False,size=10)\n",
    "sel = np.in1d(np.array(range(len(mass_ratios))),indices)\n",
    "q_train = mass_ratios[sel]\n",
    "q_val = mass_ratios[~sel]\n",
    "E_train = energies[sel]\n",
    "E_val = energies[~sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we interpolate, we should whiten our data by removing the mean and normalizing by the standard deviation.  We can also normalize q_train to be between 0 and 1.  This is a very helpful move in general, because that way we can use the same hyperparameter bounds in all interpolations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E_train_mean = np.mean(E_train)\n",
    "E_train_std = np.std(E_train)\n",
    "E_train_whitened = ((E_train-E_train_mean)/E_train_std).reshape(-1,1)\n",
    "E_val_whitened = ((E_val-E_train_mean)/E_train_std).reshape(-1,1)\n",
    "q_train_min = np.amin(q_train)\n",
    "q_train_max = np.amax(q_train)\n",
    "q_train_whitened = ((q_train-q_train_min)/(q_train_max-q_train_min)).reshape(-1,1)\n",
    "q_val_whitened = ((q_val-q_train_min)/(q_train_max-q_train_min)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the GP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = 3, 20\n",
    "theta_means = [0.0,0.0,-4.]\n",
    "theta_widths = [2,2,4]\n",
    "\n",
    "sampler = mc.EnsembleSampler(nwalkers, ndim, log_hyperposterior, \n",
    "                             args=(q_train_whitened, E_train_whitened,theta_means,theta_widths,1,True,'SE','box'))\n",
    "\n",
    "pos = [[0.,0.,-2.] + np.random.randn(ndim) for i in range(nwalkers)]\n",
    "sampler.run_mcmc(pos, 500);\n",
    "import corner\n",
    "samples = sampler.chain[:, 200:, :].reshape((-1, ndim))\n",
    "corner.corner(samples, labels=[\"$\\log_{10}\\sigma^2$\", \"$\\log_{10} l^2$\",\"$\\log_{10}$ nugget\"]);\n",
    "\n",
    "# run the optimizer too to find the max hyperposterior.  We'll do a few restarts\n",
    "# since the optimizer can misbehave\n",
    "\n",
    "reslist = []\n",
    "minhpi = None\n",
    "minhp = np.inf\n",
    "for i in range(20):\n",
    "    res = spo.minimize(log_hyperposterior,x0=[0.0,-1.,-2]+ np.random.randn(ndim),\n",
    "                       args=(q_train_whitened, E_train_whitened,theta_means,theta_widths,-1.,True,'SE','box'),\n",
    "                       bounds=([-2,2],[-2,2],[-8,0]))\n",
    "    reslist.append(res)\n",
    "    if res[\"fun\"]< minhp:\n",
    "        minhp = res[\"fun\"]\n",
    "        minhpi = i\n",
    "print reslist[minhpi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = q_train_whitened\n",
    "f = E_train_whitened\n",
    "Xstar = np.linspace(-1.,2.,500).reshape(-1,1)\n",
    "#Xstar = np.array(sorted(q_val_whitened)).reshape(-1,1)\n",
    "# Use optimized hyperparameters\n",
    "reslist[minhpi]\n",
    "cov_scale = 10**res[\"x\"][0]\n",
    "length_scale = 10**res[\"x\"][1]\n",
    "nugget_val = 10**res[\"x\"][2]\n",
    "nugget = np.diag(nugget_val*np.ones(X.shape[0]))\n",
    "KXX = cov_scale*squared_exponential_kernel(X,X,scale=length_scale,metric=None)\n",
    "KXXstar = cov_scale*squared_exponential_kernel(Xstar,X,scale=length_scale,metric=None)\n",
    "\n",
    "# Steps 1-2\n",
    "#L = spl.cholesky(KXX+nugget,lower=True)\n",
    "L = np.linalg.cholesky(KXX+nugget)\n",
    "alpha = spl.cho_solve((L,True), f)\n",
    "# Steps 3-5\n",
    "fstar_bar = np.dot(KXXstar,alpha)\n",
    "variances = []\n",
    "for i in range(KXXstar.shape[0]):\n",
    "    v = spl.solve_triangular(L,KXXstar[i,:],lower=True)\n",
    "    variances.append(cov_scale-np.dot(v.T,v))\n",
    "stds = np.sqrt(variances)\n",
    "# Step 6\n",
    "lml = -(1./2.)*np.dot(f.T,alpha) - np.sum(np.log(np.diag(L))) - (X.shape[0]/2.)*np.log(2*np.pi)\n",
    "lml = lml.flatten()[0]\n",
    "print 'The log likelihood is',lml\n",
    "\n",
    "plt.plot(q_train,E_train,'ro',label='training')\n",
    "plt.scatter(q_val,E_val,color='b',label='validation')\n",
    "qstar = Xstar*(q_train_max-q_train_min)+q_train_min\n",
    "Estar_bar = fstar_bar*E_train_std+E_train_mean\n",
    "plt.plot(qstar,Estar_bar,label='GP mean')\n",
    "qstar_flat,Estar_bar_flat=qstar.flatten(),Estar_bar.flatten()\n",
    "stds = stds*E_train_std\n",
    "plt.fill_between(qstar_flat,Estar_bar_flat-stds,Estar_bar_flat+stds,alpha=0.5,color='gray')\n",
    "#plt.ylim([-5,5])\n",
    "plt.xlabel('mass ratio')\n",
    "plt.ylabel('1 - remnant mass / final mass')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's compare our GP uncertainties to the residuals with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = q_train_whitened\n",
    "f = E_train_whitened\n",
    "Xstar = q_val_whitened\n",
    "# Use optimized hyperparameters\n",
    "\n",
    "cov_scale = 10**0.7\n",
    "length_scale = 10**-0.6\n",
    "nugget_val = 10**-5.6\n",
    "nugget = np.diag(nugget_val*np.ones(X.shape[0]))\n",
    "KXX = cov_scale*squared_exponential_kernel(X,X,scale=length_scale,metric=None)\n",
    "KXXstar = cov_scale*squared_exponential_kernel(Xstar,X,scale=length_scale,metric=None)\n",
    "\n",
    "# Steps 1-2\n",
    "#L = spl.cholesky(KXX+nugget,lower=True)\n",
    "L = np.linalg.cholesky(KXX+nugget)\n",
    "alpha = spl.cho_solve((L,True), f)\n",
    "# Steps 3-5\n",
    "fstar_bar = np.dot(KXXstar,alpha)\n",
    "variances = []\n",
    "for i in range(KXXstar.shape[0]):\n",
    "    v = spl.solve_triangular(L,KXXstar[i,:],lower=True)\n",
    "    variances.append(cov_scale-np.dot(v.T,v))\n",
    "stds = np.sqrt(variances)\n",
    "print stds\n",
    "# Step 6\n",
    "lml = -(1./2.)*np.dot(f.T,alpha) - np.sum(np.log(np.diag(L))) - (X.shape[0]/2.)*np.log(2*np.pi)\n",
    "lml = lml.flatten()[0]\n",
    "print 'The log likelihood is',lml\n",
    "\n",
    "print fstar_bar - E_val_whitened\n",
    "plt.scatter(Xstar,np.absolute(fstar_bar - E_val_whitened).flatten()/stds,label='mean residual')\n",
    "#plt.scatter(Xstar,stds,label='stds',color='r')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylim([1e-1,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Investigations\n",
    "* How do the residuals look?\n",
    "* If the residuals don't match the expected uncertainties, what changes could you make to e.g. your kernel in order to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
